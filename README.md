
# TensorFlow_Engineering_Implementation
The source code and dataset about &lt;Deep Learning - Best Practices on TensorFlow Engineering Implementation> 

![Image text](https://github.com/aianaconda/TensorFlow_Engineering_Implementation/blob/master/2.jpg)

# 《深度学习之TensorFlow工程化项目实战》一书配套代码及数据集资源(数据集太大，请访问www.aianaconda.com进行下载)
这本AI技术工具书非常专注于实战应用。书中涵盖了TensorFlow 1.x到2.0版本的各种使用说明，及开发技巧。是代码医生工作室将近几年的经验积累。
本书全长740多页，共75个实例。从基础的静态图讲到动态图再到编译子图、从估算器讲到特征列再到TFTS、从原生的TF语法讲到tf.keras使用技巧、从tfRecorder的数据集制作讲到Dataset接口的使用、还有TF_serving、saved_model、TF_lite、cleverhans等更多接口的使用介绍。

##  一、本书的特点总结如下：
###  (1)应用场景相关
从应用场景方面，本书包含了：图像识别、微调模型、目标识别、基于像素级的语义分割、文本分类、特征工程、数值分析、时间序列数据分析、特征预处理、探索性数据分析、知识图谱、机器翻译、对话机器人、推荐系统、语音合成、生成文本、序列样本的生成、图像到文本的跨域生成、预测性维护任务、清晰图像的生成、多属性图像的合成、AI模型的攻击与防护、基于URL、安卓、IOS的上的布署。
### （2）技术相关
从深度学习的技术方面，本书也包含了很多优秀模型，其中包括深度卷积、空洞卷积、胶囊卷积网络、矩阵胶囊、Mask R-CNN 、YOLO V3、PNASNet 、QRNN 、SRU、 IndRnn、IndyLSTM、 JANET等。同时也引用了大量的论文（30多篇），方便读者进行扩展阅读。
### （3）与之前书籍的承接关系（技术更新，更丰富、内容更实战）
本书可以算作是《深度学习之TensorFlow：入门、原理与进阶实战》一书的后续版本。
知识一脉相承，前后呼应。在《深度学习之TensorFlow：入门、原理与进阶实战》一书的基础之上，又更新了一些有用的技巧，其中包括：

* 更实用的数据集案例以及TFDS接口介绍
* 动态图里、静态图、估算器间的相互转化及嵌套实例
* TensorFlow1.x与TensorFlow2.x之间的关系，及转化实例
* 特征工程相关知识，以及推荐系统、知识图谱配合深度学习的应用
*	升级版的dropout（Targeted Dropout）
* 各种注意力（多头注意力、BahdanauAttention 、LuongAttention、 单调注意力机制、混合注意力机制）
*	各种归一化（ReNorm、LayerNorm、instance_norm、GroupNorm、SwitchableNorm）
* 在RNN网络中添加多项式分布
* Seq2Seq新框架更全面的介绍，和更底层的使用实例。
* 基于AI模型的安全技术（FGSM、黑箱攻击等方法）
* 图像合成、声音合成、安全领域的攻击样本合成相关实例
* 增加了基于URL、安卓、IOS的上的布署实例
##  二、书中包含的知识点如下： 
TF-slim、TF-Hub、T2T、tf.layers、tf.js、TFDS、tf.Keras、TFLearn、tfdbg、Training Hooks、Estimators、eager、TF_CONFIG、KubeFlow、tf.feature_column、知识图谱、sequence_feature_column、TFBT、 factorization、Lattice、tf.Transform、点阵校准模型、wals kmeans  BoostedTrees、深度卷积、空洞卷积、深度可分离卷积、胶囊卷积网络、矩阵胶囊、TextCnn、ResNet、PNASNet、VGG、YOLO V3、Mask R-CNN、Targeted Dropout、QRNN 、SRU、 IndRnn、IndyLSTM、 JANET、 Seq2Seq、TFTS 、多项式分布、Tacotron、TFGan、多头注意力、BahdanauAttention 、LuongAttention、 单调注意力机制、混合注意力机制、stft、ReNorm、LayerNorm、instance_norm、GroupNorm、SwitchableNorm、FGSM、cleverhans 黑箱攻击  Jacobian矩阵、defun、TF_serving  saved_model  TF_lite。
![Image text](https://github.com/aianaconda/TensorFlow_Engineering_Implementation/blob/master/36.jpg)

同样该书仍然保持以前一贯的作风：
*	组建qq群，由作者亲自解答问题
*	在大蛇智能官网上同步勘误（www.aianaconda.com）
*	开放大蛇智能论坛，方便读者交流和查阅历史问题(bbs.aianaconda.com)
*	开源图书配套的全部代码，与数据集(数据集太大，请访问www.aianaconda.com进行下载)
*	在相约机器人公众号上，持续更新跟书籍知识相关的AI扩展技术。
![Image text](https://github.com/aianaconda/TensorFlow_Engineering_Implementation/blob/master/1.jpg)


#  目录
` ` `
第1篇  准备
第1章
学习准备
1.1  TensorFlow能做什么
1.2  学习TensorFlow的必备知识
1.3  学习技巧：跟读代码
1.4  如何学习本书
第2章
搭建开发环境
2.1  准备硬件环境
2.2  下载及安装Anaconda
2.3  安装TensorFlow
2.4  GPU版本的安装方法
2.4.1  在Windows中安装CUDA
2.4.2  在Linux中安装CUDA
2.4.3  在Windows中安装cuDNN
2.4.4  在Linux中安装cuDNN
2.4.5  常见错误及解决方案
2.5  测试显卡的常用命令
2.6  TensorFlow 1.x版本与2.x版本共存的解决方案
第3章
实例1：用AI模型识别图像是桌子、猫、狗，还是其他
3.1  准备代码环境和预训练模型
3.2  代码实现：初始化环境变量，并载入ImgNet标签
3.3  代码实现：定义网络结构
3.4  代码实现：载入模型进行识别
3.5  扩展：用更多预训练模型完成图片分类任务
第2篇  基础
第4章
用TensorFlow制作自己的数据集
4.1  快速导读
4.1.1  什么是数据集
4.1.2  TensorFlow的框架
4.1.3  什么是TFDS
4.2  实例2：将模拟数据制作成内存对象数据集
4.2.1  代码实现：生成模拟数据
4.2.2  代码实现：定义占位符
4.2.3  代码实现：建立会话，并获取数据
4.2.4  代码实现：模拟数据可视化
4.2.5  运行程序
4.2.6  代码实现：创建带有迭代值并支持乱序功能的模拟数据集
4.3  实例3：将图片制作成内存对象数据集
4.3.1  样本介绍
4.3.2  代码实现：载入文件名称与标签
4.3.3  代码实现：生成队列中的批次样本数据
4.3.4  代码实现：在会话中使用数据集
4.3.5  运行程序
4.4  实例4：将Excel文件制作成内存对象数据集
4.4.1  样本介绍
4.4.2  代码实现：逐行读取数据并分离标签
4.4.3  代码实现：生成队列中的批次样本数据
4.4.4  代码实现：在会话中使用数据集
4.4.5  运行程序
4.5  实例5：将图片文件制作成TFRecord数据集
4.5.1  样本介绍
4.5.2  代码实现：读取样本文件的目录及标签
4.5.3  代码实现：定义函数生成TFRecord数据集
4.5.4  代码实现：读取TFRecord数据集，并将其转化为队列
4.5.5  代码实现：建立会话，将数据保存到文件
4.5.6  运行程序
4.6  实例6：将内存对象制作成Dataset数据集
4.6.1  如何生成Dataset数据集
4.6.2  如何使用Dataset接口
4.6.3  tf.data.Dataset接口所支持的数据集变换操作
4.6.4  代码实现：以元组和字典的方式生成Dataset对象
4.6.5  代码实现：对Dataset对象中的样本进行变换操作
4.6.6  代码实现：创建Dataset迭代器
4.6.7  代码实现：在会话中取出数据
4.6.8  运行程序
4.6.9  使用tf.data.Dataset.from_tensor_slices接口的注意事项
4.7  实例7：将图片文件制作成Dataset数据集
4.7.1  代码实现：读取样本文件的目录及标签
4.7.2  代码实现：定义函数，实现图片转换操作
4.7.3  代码实现：用自定义函数实现图片归一化
4.7.4  代码实现：用第三方函数将图片旋转30°
4.7.5  代码实现：定义函数，生成Dataset对象
4.7.6  代码实现：建立会话输出数据
4.7.7  运行程序
4.8  实例8：将tfRecord文件制作成Dataset数据集
4.8.1  样本介绍
4.8.2  代码实现：定义函数，生成Dataset对象
4.8.3  代码实现：建立会话输出数据
4.8.4  运行程序
4.9  实例9：在动态图中读取Dataset数据集
4.9.1  代码实现：添加动态图调用
4.9.2  制作数据集
4.9.3  代码实现：在动态图中显示数据
4.9.4  扩展示例10：在TensorFlow 2.x中操作数据集
4.10  实例11：在不同场景中使用数据集
4.10.1  代码实现：在训练场景中使用数据集
4.10.2  代码实现：在应用模型场景中使用数据集
4.10.3  代码实现：在训练与测试混合场景中使用数据集
4.11  tf.data.Dataset接口的更多应用
第5章
10分钟快速训练自己的图片分类模型
5.1  快速导读
5.1.1  认识模型和模型检查点文件
5.1.2  了解“预训练模型”与微调（Fine-Tune）
5.1.3  学习TensorFlow中的预训练模型库——TF-Hub库
5.2  实例12：通过微调模型分辨男女
5.2.1  准备工作
5.2.2  代码实现：处理样本数据并生成Dataset对象
5.2.3  代码实现：定义微调模型的类MyNASNetModel
5.2.4  代码实现：构建MyNASNetModel类中的基本模型
5.2.5  代码实现：实现MyNASNetModel类中的微调操作
5.2.6  代码实现：实现与训练相关的其他方法
5.2.7  代码实现：构建模型，用于训练、测试、使用
5.2.8  代码实现：通过二次迭代来训练微调模型
5.2.9  代码实现：测试模型
5.3  扩展：通过摄像头实时分辨男女
5.4  TF-slim接口中的更多成熟模型
5.5  实例13：用TF-Hub库微调模型以评估人物的年龄
5.5.1  准备样本
5.5.2  下载TF-Hub库中的模型
5.5.3  代码实现：测试TF-Hub库中的MobileNet_V2模型
5.5.4  用TF-Hub库微调MobileNet_V2模型
5.5.5  代码实现：用模型评估人物的年龄
5.5.6  扩展：用TF-Hub库中的其他模型处理不同领域的分类任务
5.6  总结
5.7  练习题
5.7.1  基于TF-slim接口的练习
5.7.2  基于TF-Hub库的练习
第6章
用TensorFlow编写训练模型的程序
6.1  快速导读
6.1.1  训练模型是怎么一回事
6.1.2  用“静态图”方式训练模型
6.1.3  用“动态图”方式训练模型
6.1.4  什么是估算器框架接口（Estimators API）
6.1.5  什么是tf.layers接口
6.1.6  什么是tf.keras接口
6.1.7  什么是tf.js接口
6.1.8  什么是TFLearn框架
6.1.9  该选择哪种框架
6.1.10  分配运算资源与使用分布策略
6.1.11  用tfdbg调试TensorFlow模型
6.1.12  用钩子函数（Training_Hooks）跟踪训练状态
6.1.13  用分布式运行方式训练模型
6.1.14  用T2T框架系统更方便地训练模型
6.1.15  将TensorFlow 1.x中的代码移植到2.x版本
6.1.16  TensorFlow 2.x中的新特性——自动图
6.2  实例14：用静态图训练一个具有保存检查点功能的回归模型
6.2.1  准备开发步骤
6.2.2  生成检查点文件
6.2.3  载入检查点文件
6.2.4  代码实现：在线性回归模型中加入保存检查点功能
6.2.5  修改迭代次数，二次训练
6.3  实例15：用动态图（eager）训练一个具有保存检查点功能的回归模型
6.3.1  代码实现：启动动态图，生成模拟数据
6.3.2  代码实现：定义动态图的网络结构
6.3.3  代码实现：在动态图中加入保存检查点功能
6.3.4  代码实现：按指定迭代次数进行训练，并可视化结果
6.3.5  运行程序，显示结果
6.3.6  代码实现：用另一种方法计算动态图梯度
6.3.7  扩展实例16：在动态图中获取参数变量
6.3.8  小心动态图中的参数陷阱
6.3.9  扩展实例17：在静态图中使用动态图
6.4  实例18：用估算器框架训练一个回归模型
6.4.1  代码实现：生成样本数据集
6.4.2  代码实现：设置日志级别
6.4.3  代码实现：实现估算器的输入函数
6.4.4  代码实现：定义估算器的模型函数
6.4.5  代码实现：通过创建config文件指定硬件的运算资源
6.4.6  代码实现：定义估算器
6.4.7  用tf.estimator.RunConfig控制更多的训练细节
6.4.8  代码实现：用估算器训练模型
6.4.9  代码实现：通过热启动实现模型微调
6.4.10  代码实现：测试估算器模型
6.4.11  代码实现：使用估算器模型
6.4.12  扩展实例19：为估算器添加日志钩子函数
6.5  实例20：将估算器代码改写成静态图代码
6.5.1  代码实现：复制网络结构
6.5.2  代码实现：重用输入函数
6.5.3  代码实现：创建会话恢复模型
6.5.4  代码实现：继续训练
6.6  实例21：用tf.layers API在动态图上识别手写数字
6.6.1  代码实现：启动动态图并加载手写图片数据集
6.6.2  代码实现：定义模型的类
6.6.3  代码实现：定义网络的反向传播
6.6.4  代码实现：训练模型
6.7  实例22：用tf.keras API训练一个回归模型
6.7.1  代码实现：用model类搭建模型
6.7.2  代码实现：用sequential类搭建模型
6.7.3  代码实现：搭建反向传播的模型
6.7.4  代码实现：用两种方法训练模型
6.7.5  代码实现：获取模型参数
6.7.6  代码实现：测试模型与用模型进行预测
6.7.7  代码实现：保存模型与加载模型
6.7.8  代码实现：将模型导出成JSON文件，再将JSON文件导入模型
6.7.9  扩展实例23：在tf.keras接口中使用预训练模型ResNet
6.7.10  扩展：在动态图中使用tf.keras接口
6.7.11  扩展实例24：在静态图中使用tf.keras接口
6.8  实例25：用tf.js接口后方训练一个回归模型
6.8.1  代码实现：在HTTP的头标签中添加tfjs模块
6.8.2  代码实现：用JavaScript脚本实现回归模型
6.8.3  运行程序：在浏览器中查看效果
6.8.4  扩展：tf.js 接口的应用场景
6.9  实例26：用估算器框架实现分布式部署训练
6.9.1  运行程序：修改估算器模型，使其支持分布式
6.9.2  通过TF_CONFIG进行分布式配置
6.9.3  运行程序
6.9.4  扩展：用分布策略或KubeFlow框架进行分布式部署
6.10  实例27：在分布式估算器框架中用tf.keras接口训练ResNet模型，识别图片中是桔子还是苹果
6.10.1  样本准备
6.10.2  代码实现：准备训练与测试数据集
6.10.3  代码实现：制作模型输入函数
6.10.4  代码实现：搭建ResNet模型
6.10.5  代码实现：训练分类器模型
6.10.6  运行程序：评估模型
6.10.7  扩展：全连接网络的优化
6.11  实例28：在T2T框架中用tf.layers接口实现MNIST数据集分类
6.11.1  代码实现：查看T2T框架中的数据集（problems）
6.11.2  代码实现：构建T2T框架的工作路径及下载数据集
6.11.3  代码实现：在T2T框架中搭建自定义卷积网络模型
6.11.4  代码实现：用动态图方式训练自定义模型
6.11.5  代码实现：在动态图中用metrics模块评估模型
6.12  实例29：在T2T框架中，用自定义数据集训练中英文翻译模型
6.12.1  代码实现：声明自己的problems数据集
6.12.2  代码实现：定义自己的problems数据集
6.12.3  在命令行下生成tfrecoder格式的数据
6.12.4  查找T2T框架中的模型及超参，并用指定的模型及超参进行训练
6.12.5  用训练好的T2T框架模型进行预测
6.12.6  扩展：在T2T框架中，如何选取合适的模型及超参
6.13  实例30：将TensorFlow 1.x中的代码升级为可用于2.x版本的代码
6.13.1  准备工作：创建Python虚环境
6.13.2  使用工具转换源码
6.13.3  修改转换后的代码文件
6.13.4  将代码升级到TensorFlow 2.x版本的经验总结
第3篇  进阶
第7章
特征工程——会说话的数据
7.1  快速导读
7.1.1  特征工程的基础知识
7.1.2  离散数据特征与连续数据特征
7.1.3  了解特征列接口
7.1.4  了解序列特征列接口
7.1.5  了解弱学习器接口——梯度提升树（TFBT接口）
7.1.6  了解特征预处理模块（tf.Transform）
7.1.7  了解因子分解模块
7.1.8  了解加权矩阵分解算法
7.1.9  了解Lattice模块——点阵模型
7.1.10  联合训练与集成学习
7.2  实例31：用wide_deep模型预测人口收入
7.2.1  了解人口收入数据集
7.2.2  代码实现：探索性数据分析
7.2.3  认识wide_deep模型
7.2.4  部署代码文件
7.2.5  代码实现：初始化样本常量
7.2.6  代码实现：生成特征列
7.2.7  代码实现：生成估算器模型
7.2.8  代码实现：定义输入函数
7.2.9  代码实现：导出冻结图模型函数
7.2.10  代码实现：定义类，解析启动参数
7.2.11  代码实现：训练和测试模型
7.2.12  代码实现：使用模型
7.2.13  运行程序
7.3  实例32：用弱学习器中的梯度提升树算法预测人口收入
7.3.1  代码实现：为梯度提升树模型准备特征列
7.3.2  代码实现：构建梯度提升树模型
7.3.3  代码实现：训练并导出梯度提升树模型
7.3.4  代码实现：设置启动参数，运行程序
7.3.5  扩展：更灵活的TFBT接口
7.4  实例33：用feature_column模块转换特征列
7.4.1  代码实现：用feature_column模块处理连续值特征列
7.4.2  代码实现：将连续值特征列转化成离散值特征列
7.4.3  代码实现：将离散文本特征列转化为one-hot与词向量
7.4.4  代码实现：根据特征列生成交叉列
7.5  实例34：用sequence_feature_column接口完成自然语言处理任务的数据预处理工作
7.5.1  代码实现：构建模拟数据
7.5.2  代码实现：构建词嵌入初始值
7.5.3  代码实现：构建词嵌入特征列与共享特征列
7.5.4  代码实现：构建序列特征列的输入层
7.5.5  代码实现：建立会话输出结果
7.6  实例35：用factorization模块的kmeans接口聚类COCO数据集中的标注框
7.6.1  代码实现：设置要使用的数据集
7.6.2  代码实现：准备带聚类的数据样本
7.6.3  代码实现：定义聚类模型
7.6.4  代码实现：训练模型
7.6.5  代码实现：输出图示化结果
7.6.6  代码实现：提取并排序聚类结果
7.6.7  扩展：聚类与神经网络混合训练
7.7  实例36：用加权矩阵分解模型实现基于电影评分的推荐系统
7.7.1  下载并加载数据集
7.7.2  代码实现：根据用户和电影特征列生成稀疏矩阵
7.7.3  代码实现：建立WALS模型，并对其进行训练
7.7.4  代码实现：评估WALS模型
7.7.5  代码实现：用WALS模型为用户推荐电影
7.7.6  扩展：使用WALS的估算器接口
7.8  实例37：用Lattice模块预测人口收入
7.8.1  代码实现：读取样本，并创建输入函数
7.8.2  代码实现：创建特征列，并保存校准关键点
7.8.3  代码实现：创建校准线性模型
7.8.4  代码实现：创建校准点阵模型
7.8.5  代码实现：创建随机微点阵模型
7.8.6  代码实现：创建集合的微点阵模型
7.8.7  代码实现：定义评估与训练函数
7.8.8  代码实现：训练并评估模型
7.8.9  扩展实例38：将点阵模型嵌入神经网络中
7.9  实例38：结合知识图谱实现基于电影的推荐系统
7.9.1  准备数据集
7.9.2  预处理数据
7.9.3  搭建MKR模型
7.9.4  训练模型并输出结果
7.10  总结：可解释性算法的意义
第8章
卷积神经网络（CNN）——在图像处理中应用最广泛的模型
8.1  快速导读
8.1.1  认识卷积神经网络
8.1.2  什么是空洞卷积
8.1.3  什么是深度卷积
8.1.4  什么是深度可分离卷积
8.1.5  了解卷积网络的缺陷及补救方法
8.1.6  了解胶囊神经网络与动态路由
8.1.7  了解矩阵胶囊网络与EM路由算法
8.1.8  什么是NLP任务
8.1.9  了解多头注意力机制与内部注意力机制
8.1.10  什么是带有位置向量的词嵌入
8.1.11  什么是目标检测任务
8.1.12  什么是目标检测中的上采样与下采样
8.1.13  什么是图片分割任务
8.2  实例39：用胶囊网络识别黑白图中的服装图案
8.2.1  熟悉样本：了解Fashion-MNIST数据集
8.2.2  下载Fashion-MNIST数据集
8.2.3  代码实现：读取及显示Fashion-MNIST中的数据
8.2.4  代码实现：定义胶囊网络模型类CapsuleNetModel
8.2.5  代码实现：实现胶囊网络的基本结构
8.2.6  代码实现：构建胶囊网络模型
8.2.7  代码实现：载入数据集，并训练胶囊网络模型
8.2.8  代码实现：建立会话训练模型
8.2.9  运行程序
8.2.10  扩展实例40：实现带有EM路由的胶囊网络
8.3  实例41：用TextCNN模型分析评论者是否满意
8.3.1  熟悉样本：了解电影评论数据集
8.3.2  熟悉模型：了解TextCNN模型
8.3.3  数据预处理：用preprocessing接口制作字典
8.3.4  代码实现：生成NLP文本数据集
8.3.5  代码实现：定义TextCNN模型
8.3.6  代码实现：训练TextCNN模型
8.3.7  运行程序
8.3.8  扩展：提升模型精度的其他方法
8.4  实例42：用带注意力机制的模型分析评论者是否满意
8.4.1  熟悉样本：了解tf.keras接口中的电影评论数据集
8.4.2  代码实现：将tf.keras接口中的IMDB数据集还原成句子
8.4.3  代码实现：用tf.keras接口开发带有位置向量的词嵌入层
8.4.4  代码实现：用tf.keras接口开发注意力层
8.4.5  代码实现：用tf.keras接口训练模型
8.4.6  运行程序
8.4.7  扩展：用Targeted Dropout技术进一步提升模型的性能
8.5  实例43：搭建YOLO V3模型，识别图片中的酒杯、水果等物体
8.5.1  YOLO V3模型的样本与结构
8.5.2  代码实现：Darknet-53 模型的darknet块
8.5.3  代码实现：Darknet-53 模型的下采样卷积
8.5.4  代码实现：搭建Darknet-53模型，并返回3种尺度特征值
8.5.5  代码实现：定义YOLO检测模块的参数及候选框
8.5.6  代码实现：定义YOLO检测块，进行多尺度特征融合
8.5.7  代码实现：将YOLO检测块的特征转化为bbox attrs单元
8.5.8  代码实现：实现YOLO V3的检测部分
8.5.9  代码实现：用非极大值抑制算法对检测结果去重
8.5.10  代码实现：载入预训练权重
8.5.11  代码实现：载入图片，进行目标实物的识别
8.5.12  运行程序
8.6  实例44：用YOLO V3模型识别门牌号
8.6.1  工程部署：准备样本
8.6.2  代码实现：读取样本数据，并制作标签
8.6.3  代码实现：用tf.keras接口构建YOLO V3模型，并计算损失
8.6.4  代码实现：在动态图中训练模型
8.6.5  代码实现：用模型识别门牌号
8.6.6  扩展：标注自己的样本
8.7  实例45：用Mask R-CNN模型定位物体的像素点
8.7.1  下载COCO数据集及安装pycocotools
8.7.2  代码实现：验证pycocotools及读取COCO数据集
8.7.3  拆分Mask R-CNN模型的处理步骤
8.7.4  工程部署：准备代码文件及模型
8.7.5  代码实现：加载数据创建模型，并输出模型权重
8.7.6  代码实现：搭建残差网络ResNet
8.7.7  代码实现：搭建Mask R-CNN模型的骨干网络ResNet
8.7.8  代码实现：可视化Mask R-CNN模型骨干网络的特征输出
8.7.9  代码实现：用特征金字塔网络处理骨干网络特征
8.7.10  计算RPN中的锚点
8.7.11  代码实现：构建RPN
8.7.12  代码实现：用非极大值抑制算法处理RPN的结果
8.7.13  代码实现：提取RPN的结果
8.7.14  代码实现：可视化RPN的结果
8.7.15  代码实现：在MaskRCNN类中对ROI区域进行分类
8.7.16  代码实现：金字塔网络的区域对齐层（ROIAlign）中的区域框与特征的匹配算法
8.7.17  代码实现：在金字塔网络的ROIAlign层中按区域边框提取内容
8.7.18  代码实现：调试并输出ROIAlign层的内部运算值
8.7.19  代码实现：对ROI内容进行分类
8.7.20  代码实现：用检测器DetectionLayer检测ROI内容，得到最终的实物矩形
8.7.21  代码实现：根据ROI内容进行实物像素分割
8.7.22  代码实现：用Mask R-CNN模型分析图片
8.8  实例46：训练Mask R-CNN模型，进行形状的识别
8.8.1  工程部署：准备代码文件及模型
8.8.2  样本准备：生成随机形状图片
8.8.3  代码实现：为Mask R-CNN模型添加损失函数
8.8.4  代码实现：为Mask R-CNN模型添加训练函数，使其支持微调与全网训练
8.8.5  代码实现：训练并使用模型
8.8.6  扩展：替换特征提取网络
第9章
循环神经网络（RNN）——处理序列样本的神经网络
9.1  快速导读
9.1.1  什么是循环神经网络
9.1.2  了解RNN模型的基础单元LSTM与GRU
9.1.3  认识QRNN单元
9.1.4  认识SRU单元
9.1.5  认识IndRNN单元
9.1.6  认识JANET单元
9.1.7  优化RNN模型的技巧
9.1.8  了解RNN模型中多项式分布的应用
9.1.9  了解注意力机制的Seq2Seq框架
9.1.10  了解BahdanauAttention与LuongAttention
9.1.11  了解单调注意力机制
9.1.12  了解混合注意力机制
9.1.13  了解Seq2Seq接口中的采样接口（Helper）
9.1.14  了解RNN模型的Wrapper接口
9.1.15  什么是时间序列（TFTS）框架
9.1.16  什么是梅尔标度
9.1.17  什么是短时傅里叶变换
9.2  实例47：搭建RNN模型，为女孩生成英文名字
9.2.1  代码实现：读取及处理样本
9.2.2  代码实现：构建Dataset数据集
9.2.3  代码实现：用tf.keras接口构建生成式RNN模型
9.2.4  代码实现：在动态图中训练模型
9.2.5  代码实现：载入检查点文件并用模型生成名字
9.2.6  扩展：用RNN模型编写文章
9.3  实例48：用带注意力机制的Seq2Seq模型为图片添加内容描述
9.3.1  设计基于图片的Seq2Seq
9.3.2  代码实现：图片预处理——用ResNet提取图片特征并保存
9.3.3  代码实现：文本预处理——过滤处理、字典建立、对齐与向量化处理
9.3.4  代码实现：创建数据集
9.3.5  代码实现：用tf.keras接口构建Seq2Seq模型中的编码器
9.3.6  代码实现：用tf.keras接口构建Bahdanau类型的注意力机制
9.3.7  代码实现：搭建Seq2Seq模型中的解码器Decoder
9.3.8  代码实现：在动态图中计算Seq2Seq模型的梯度
9.3.9  代码实现：在动态图中为Seq2Seq模型添加保存检查点功能
9.3.10  代码实现：在动态图中训练Seq2Seq模型
9.3.11  代码实现：用多项式分布采样获取图片的内容描述
9.4  实例49：用IndRNN与IndyLSTM单元制作聊天机器人
9.4.1  下载及处理样本
9.4.2  代码实现：读取样本，分词并创建字典
9.4.3  代码实现：对样本进行向量化、对齐、填充预处理
9.4.4  代码实现：在Seq2Seq模型中加工样本
9.4.5  代码实现：在Seq2Seq模型中，实现基于IndRNN与IndyLSTM的动态多层RNN编码器
9.4.6  代码实现：为Seq2Seq模型中的解码器创建Helper
9.4.7  代码实现：实现带有Bahdanau注意力、dropout、OutputProjectionWrapper的解码器
9.4.8  代码实现：在Seq2Seq模型中实现反向优化
9.4.9  代码实现：创建带有钩子函数的估算器，并进行训练
9.4.10  代码实现：用估算器框架评估模型
9.4.11  扩展：用注意力机制的Seq2Seq模型实现中英翻译
9.5  实例50：预测飞机发动机的剩余使用寿命
9.5.1  准备样本
9.5.2  代码实现：预处理数据——制作数据集的输入样本与标签
9.5.3  代码实现：构建带有JANET单元的多层动态RNN模型
9.5.4  代码实现：训练并测试模型
9.5.5  运行程序
9.5.6  扩展：为含有JANET单元的RNN模型添加注意力机制
9.6  实例51：将动态路由用于RNN模型，对路透社新闻进行分类
9.6.1  准备样本
9.6.2  代码实现：预处理数据——对齐序列数据并计算长度
9.6.3  代码实现：定义数据集
9.6.4  代码实现：用动态路由算法聚合信息
9.6.5  代码实现：用IndyLSTM单元搭建RNN模型
9.6.6  代码实现：建立会话，训练网络
9.6.7  扩展：用
` ` `
